{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV as gridsearchcv\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "import os\n",
    "\n",
    "# Included following due to internet certificate problems\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Move to correct folder for server.  Can remove before sending\n",
    "# os.chdir('/home/poblivsig/Dropbox/horses2')\n",
    "os.chdir('/Users/paullivesey/Dropbox/2. Personal/3. Projects/Python/unsupervised')\n",
    "\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Open the pre-processed csv\n",
    "df = pd.read_csv('data/winequality-red.csv')\n",
    "# df = pd.read_csv('data/phishing.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Get info about wine\n",
    "print(f'Shape\\n\\n{df.shape}')\n",
    "print(f'Columns\\n\\n{df.columns}')\n",
    "print(f'dtypes\\n\\n{df.dtypes}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(f'Description\\n\\n{df.describe()}')\n",
    "print(f'Info:\\n{df.info}')\n",
    "print(f'Check out the sample: {df.sample(n=1)}')\n",
    "pd.set_option('display.max_columns', 5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df['quality']\n",
    "X = df.drop('quality', axis=1)\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10\n",
    "GM_N_CLUSTERS = 10\n",
    "INIT = 'k-means++'\n",
    "N_INIT = 10\n",
    "KM_MAX_ITERS = 300\n",
    "TOLERANCE = 1e-4\n",
    "PC_DISTANCES = True\n",
    "KM_VERBOSE = 0\n",
    "KM_RANDOM_STATE = 42\n",
    "ALGORITHM = 'full'\n",
    "FEATURE_1_TO_PLOT = 8\n",
    "FEATURE_2_TO_PLOT = 9\n",
    "N_COMPONENTS = 11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Constants for different algorithms\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the features (attributes)\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualization of the raw data\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('bmh')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10\n",
    "silhouettes = []\n",
    "\n",
    "## Loop through the cluster numbers and output silhouette\n",
    "## and elbow charts\n",
    "\n",
    "for n_cluster in range(2, N_CLUSTERS+1):\n",
    "    km = KMeans(n_clusters=n_cluster,\n",
    "                init=INIT,\n",
    "                n_init=N_INIT,\n",
    "                max_iter=KM_MAX_ITERS,\n",
    "                tol=TOLERANCE,\n",
    "                precompute_distances=PC_DISTANCES,\n",
    "                verbose=KM_VERBOSE,\n",
    "                random_state=KM_RANDOM_STATE,\n",
    "                algorithm=ALGORITHM)\n",
    "\n",
    "    y_pred = km.fit_predict(X)\n",
    "\n",
    "    ### Print some stats\n",
    "    print(f'inertia = {km.inertia_}')\n",
    "    silhouettes.append(silhouette_score(X, km.labels_, metric='euclidean'))\n",
    "    # print(f'silhouette score = {s_score:.3f}')\n",
    "\n",
    "print(f'silhouettes = {silhouettes}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(x=np.arange(2 ,N_CLUSTERS+1), y=silhouettes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kmeans(Xk, xlim, ylim, data_title):\n",
    "    print(len(np.arange(2, N_CLUSTERS+1)))\n",
    "    print(len(silhouettes))\n",
    "\n",
    "    #%\n",
    "    #****** Run the KMeans and create Silhouette and scatter ******\n",
    "    clusters = np.arange(2, N_CLUSTERS+1)\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    ## Borrowed from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "    for cluster in clusters:\n",
    "        ## Build the plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ## The plot on x for the silhouette coeffients ranges from -1 to +1\n",
    "        ax1.set_xlim([-0.25, 1])\n",
    "        ## The plot on y has to include all of the shapes with their values sorted\n",
    "        ax1.set_ylim([0, len(X) + (cluster + 1) * 10])\n",
    "        fig.set_size_inches(16, 6)\n",
    "\n",
    "        ## Now run the clustering algorithm itself\n",
    "        km = KMeans(n_clusters=cluster,\n",
    "                    init=INIT,\n",
    "                    n_init=N_INIT,\n",
    "                    max_iter=KM_MAX_ITERS,\n",
    "                    tol=TOLERANCE,\n",
    "                    precompute_distances=PC_DISTANCES,\n",
    "                    verbose=KM_VERBOSE,\n",
    "                    random_state=KM_RANDOM_STATE,\n",
    "                    algorithm=ALGORITHM)\n",
    "\n",
    "        print(f'inertia for {cluster} clusters = {km.inertia_}')\n",
    "        y_pred = km.fit_predict(Xk)\n",
    "        cluster_lbls = km.labels_\n",
    "\n",
    "        ## Get the silhoueete score which gives a basic silhouette_score\n",
    "        ## for the run.  Store away from plotting later\n",
    "        silhouette_average = silhouette_score(Xk, y_pred)\n",
    "        silhouette_scores[cluster] = silhouette_average\n",
    "        # What is the silhouette score for each instance?\n",
    "        sample_silhouette_scores = silhouette_samples(Xk, y_pred)\n",
    "\n",
    "        lower_y = 10\n",
    "        for j in range(cluster):\n",
    "            # Group together the silhouette coefficients for cluster i\n",
    "            # and the sort them from largest to smallest\n",
    "            j_cluster_coeffs = sample_silhouette_scores[y_pred == j]\n",
    "            j_cluster_coeffs.sort()\n",
    "\n",
    "            ## Get bottom of cluster shape for chart\n",
    "            upper_y = lower_y + j_cluster_coeffs.shape[0]\n",
    "            colour = cm.rainbow(float(j) / cluster)\n",
    "\n",
    "            ## Draw the cluster shape\n",
    "            ax1.fill_betweenx(np.arange(lower_y, upper_y),\n",
    "                             0, j_cluster_coeffs,\n",
    "                             facecolor=colour, edgecolor=colour, alpha=0.7)\n",
    "            ax1.text(-0.05, lower_y + 0.5 *j_cluster_coeffs.shape[0], str(j))\n",
    "\n",
    "            # Get the next clusters position\n",
    "            lower_y = upper_y + 10\n",
    "\n",
    "        ## Draw the average silhouette score line.\n",
    "        ax1.axvline(x=silhouette_average, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "        ## Set the title and labels\n",
    "        ax1.set_xlabel('Silhouette Coefficient', fontsize=11)\n",
    "        ax1.set_ylabel('Cluster', fontsize=11)\n",
    "\n",
    "        ax1.set_title(f'Silhouette Diagram for {cluster} Clusters', fontsize=14)\n",
    "\n",
    "        ## Create 2D scatterplot for the clusters created above\n",
    "        ax2.scatter( Xk[:, FEATURE_1_TO_PLOT],\n",
    "                     Xk[:, FEATURE_2_TO_PLOT],\n",
    "                     marker='.',\n",
    "                     s=30,\n",
    "                     lw=0,\n",
    "                     alpha=0.5,\n",
    "                     c=cm.rainbow(km.labels_.astype(float) / cluster),\n",
    "                     edgecolor='k')\n",
    "        ax2.scatter(km.cluster_centers_[:, 0],\n",
    "                    km.cluster_centers_[:, 1],\n",
    "                    marker='o',\n",
    "                    c='white',\n",
    "                    alpha=1,\n",
    "                    s=180,\n",
    "                    edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(km.cluster_centers_):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "        ax2.set_xlim([0, xlim])\n",
    "        ax2.set_xlim([0, ylim])\n",
    "        ax2.set_xlabel('1st Feature')\n",
    "        ax2.set_ylabel('2nd Feature')\n",
    "        ax2.set_title(f'{cluster} Cluster data scatterplot for 2 Features.', fontsize=14)\n",
    "\n",
    "        plt.suptitle((f'{data_title} K-Means Clustering on Sample Data with {cluster} Clusters'),\n",
    "                     fontsize=15)\n",
    "\n",
    "    # print(f'silhouette scores = {silhouette_scores}')\n",
    "    plt.show()\n",
    "    print(f'silhouettes = {silhouettes}')\n",
    "\n",
    "    plt.title(f'Silhouette Line-plot for {data_title}', fontsize=14)\n",
    "    plt.xlabel('Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.plot(np.arange(1, 10),silhouettes)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#g****** Run the GAUSSIAN MIXTURE and create Silhouette and scatter ******\n",
    "clusters = np.arange(2, N_CLUSTERS+1)\n",
    "silhouette_scores = {}\n",
    "bics = []\n",
    "aics = []\n",
    "\n",
    "## Borrowed from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "for cluster in clusters:\n",
    "    ## Build the plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ## The plot on x for the silhouette coeffients ranges from -1 to +1\n",
    "    ax1.set_xlim([-0.25, 1])\n",
    "    ## The plot on y has to include all of the shapes with their values sorted\n",
    "    ax1.set_ylim([0, len(X) + (cluster + 1) * 10])\n",
    "    fig.set_size_inches(16, 6)\n",
    "\n",
    "    ## Now run the clustering algorithm itself\n",
    "    gm = GaussianMixture(n_components=cluster )\n",
    "    y_pred_gm = gm.fit_predict(X)\n",
    "    bics.append(gm.bic(X))\n",
    "    aics.append(gm.aic(X))\n",
    "    # km = KMeans(n_clusters=cluster,\n",
    "    #             init=INIT,\n",
    "    #             n_init=N_INIT,\n",
    "    #             max_iter=KM_MAX_ITERS,\n",
    "    #             tol=TOLERANCE,\n",
    "    #             precompute_distances=PC_DISTANCES,\n",
    "    #             verbose=KM_VERBOSE,\n",
    "    #             random_state=KM_RANDOM_STATE,\n",
    "    #             algorithm=ALGORITHM)\n",
    "\n",
    "    # y_pred = km.fit_predict(X)\n",
    "    cluster_lbls = np.unique(y_pred_gm[:cluster]) #gm.labels_\n",
    "\n",
    "    ## Get the silhouette score which gives a basic silhouette_score\n",
    "    ## for the run.  Store away from plotting later\n",
    "    silhouette_average = silhouette_score(X, y_pred_gm)\n",
    "    silhouette_scores[cluster] = silhouette_average\n",
    "    # What is the silhouette score for each instance?\n",
    "    sample_silhouette_scores = silhouette_samples(X, y_pred_gm)\n",
    "\n",
    "    lower_y = 10\n",
    "    for j in range(cluster):\n",
    "        # Group together the silhouette coefficients for cluster i\n",
    "        # and the sort them from largest to smallest\n",
    "        j_cluster_coeffs = sample_silhouette_scores[y_pred_gm == j]\n",
    "        j_cluster_coeffs.sort()\n",
    "\n",
    "        ## Get bottom of cluster shape for chart\n",
    "        upper_y = lower_y + j_cluster_coeffs.shape[0]\n",
    "        colour = cm.rainbow(float(j) / cluster)\n",
    "\n",
    "        ## Draw the cluster shape\n",
    "        ax1.fill_betweenx(np.arange(lower_y, upper_y),\n",
    "                         0, j_cluster_coeffs,\n",
    "                         facecolor=colour, edgecolor=colour, alpha=0.7)\n",
    "        ax1.text(-0.05, lower_y + 0.5 *j_cluster_coeffs.shape[0], str(j))\n",
    "\n",
    "        # Get the next clusters position\n",
    "        lower_y = upper_y + 10\n",
    "\n",
    "    ## Draw the average silhouette score line.\n",
    "    ax1.axvline(x=silhouette_average, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "    ## Set the title and labels\n",
    "    ax1.set_xlabel('Silhouette Coefficient', fontsize=11)\n",
    "    ax1.set_ylabel('Cluster', fontsize=11)\n",
    "\n",
    "    ax1.set_title(f'Silhouette Diagram for {cluster} Clusters', fontsize=14)\n",
    "\n",
    "    ## Create 2D scatterplot for the clusters created above\n",
    "    ax2.scatter( X[:, FEATURE_1_TO_PLOT],\n",
    "                 X[:, FEATURE_2_TO_PLOT],\n",
    "                 marker='.',\n",
    "                 s=30,\n",
    "                 lw=0,\n",
    "                 alpha=0.5,\n",
    "                 c=cm.rainbow(km.labels_.astype(float) / cluster),\n",
    "                 edgecolor='k')\n",
    "    ## Find centers for Gaussian clusters (choosing the points with\n",
    "    ## the maximal density to represent its cluster.\n",
    "    centers = np.empty(shape=(gm.n_components, X.shape[1]))\n",
    "    for i in range(gm.n_components):\n",
    "        density = multivariate_normal(cov=gm.covariances_[i],\n",
    "                                      mean=gm.means_[i]).logpdf(X)\n",
    "        centers[i, :] = X[np.argmax(density)]\n",
    "\n",
    "    ax2.scatter(centers[:, 0],\n",
    "                centers[:, 1],\n",
    "                marker='o',\n",
    "                c=\"white\",\n",
    "                alpha=1,\n",
    "                s=180,\n",
    "                edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0],\n",
    "                    c[1],\n",
    "                    marker='$%d$' % i,\n",
    "                    alpha=1,\n",
    "                    s=50,\n",
    "                    edgecolor='k')\n",
    "    ax2.set_xlim([0,5])\n",
    "    ax2.set_xlabel(\"1st Feature\")\n",
    "    ax2.set_ylabel(\"2nd Feature\")\n",
    "    ax2.set_title(\"Clustered data scatterplot for 2 Features.\", fontsize=14)\n",
    "\n",
    "    plt.suptitle((f'K-Means Clustering on Sample Data with {cluster} Clusters'),\n",
    "                 fontsize=14)\n",
    "\n",
    "# print(f'silhouette scores = {silhouette_scores}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Create the Silhouette Score Chart\n",
    "sns.lineplot(x=clusters, y=list(silhouette_scores.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bics = []\n",
    "aics = []\n",
    "clusters = np.arange(2, N_CLUSTERS+1)\n",
    "\n",
    "for n_cluster in clusters:\n",
    "    gm = GaussianMixture(n_components=n_cluster )\n",
    "    y_clust_gm = gm.fit_predict(X)\n",
    "    bics.append(gm.bic(X))\n",
    "    aics.append(gm.aic(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Create AIC and BIC chart\n",
    "plt.plot(clusters, bics, label='bic')\n",
    "plt.plot(clusters, aics, label='aic')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Find the Bayesian Gaussian Mixture\n",
    "bgm = BayesianGaussianMixture ( n_components = 10 , n_init = 10 )\n",
    "bgm.fit ( X )\n",
    "print(np.round ( bgm.weights_ , 2 ))\n",
    "\n",
    "##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Calculate the best PCS dimensions\n",
    "pca_res = KernelPCA(n_components=6, kernel='rbf', degree=4, gamma=0.1)\n",
    "pca_res.fit(X)\n",
    "d = np.argmax(np.cumsum(pca_res.explained_variance_ratio_) >=  0.95) + 1\n",
    "print(f'optimal PCA dimensions = {d}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Use the optimal dimension to calculate the principal components...\n",
    "\n",
    "pca = KernelPCA(n_components=5)\n",
    "X2dim = pca.fit_transform(X)\n",
    "print(f'New dimensions = {X2dim.shape}')\n",
    "print(f'principal components = {pca.explained_variance_ratio_}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Plot different dimensions against the explained variance\n",
    "NO_DIMS_TO_CHECK = 11\n",
    "dimensions = np.arange(1, NO_DIMS_TO_CHECK)\n",
    "expl_variances = []\n",
    "\n",
    "for dimension in dimensions:\n",
    "    pca = PCA(n_components=dimension)\n",
    "    pca.fit(X)\n",
    "    expl_variances.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(f'expl_variances = \\n{expl_variances}')\n",
    "print(f'dimensions = \\n{dimensions}')\n",
    "plt.title('Explained Variance vs. Dimensions', fontsize=14)\n",
    "plt.xlabel('No. of Dimensions', fontsize=12)\n",
    "plt.ylabel('Explained Variance', fontsize=12)\n",
    "plt.axvline(6, color='r', linestyle='dotted')\n",
    "plt.axvline(6, color='r', linestyle='dotted')\n",
    "plt.axhline(0.938, color='r', linestyle='dotted')\n",
    "sns.lineplot(dimensions, expl_variances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CA_Algorithm_2D(algorithm, KPCA, PCA_TYPE, title, components, **args):\n",
    "    if KPCA:\n",
    "        result = algorithm(n_components=components,\n",
    "                           kernel=args['kernel'],\n",
    "                           degree=args['degree'],\n",
    "                           gamma=args['gamma'])\n",
    "    else:\n",
    "        result = algorithm(n_components=components) #, kernel='rbf', degree=4, gamma=0.4)\n",
    "\n",
    "    X2dim = result.fit_transform(X)\n",
    "    print(f'New dimensions = {X2dim.shape}')\n",
    "\n",
    "    QUALITY_1 = 5\n",
    "    QUALITY_2 = 6\n",
    "    QUALITY_3 = 7\n",
    "\n",
    "    qualities = [QUALITY_1,\n",
    "                 QUALITY_2,\n",
    "                 QUALITY_3]\n",
    "    plt.figure()\n",
    "    for col, j in zip (['r', 'c', 'y'], qualities):\n",
    "        plt.scatter(X2dim[y == j, 0],\n",
    "                    X2dim[y == j, 1],\n",
    "                    alpha=0.4,\n",
    "                    marker='.',\n",
    "                    label=j,\n",
    "                    color=col, #['r', 'c'],\n",
    "                    lw=2,\n",
    "                    s=40)\n",
    "    plt.legend(title='Quality', loc='best')\n",
    "    plt.xlabel('X1', fontsize=12)\n",
    "    plt.ylabel('X2', fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    if PCA_TYPE:\n",
    "        d = np.argmax(np.cumsum(result.explained_variance_ratio_) >=  0.95) + 1\n",
    "        print(f'optimal PCA dimensions = {d}')\n",
    "\n",
    "    ## Return the results to be used in other algorithms\n",
    "    return X2dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CA_Algorithm_3D(algorithm, KPCA, title, components, **args):\n",
    "    if KPCA:\n",
    "        result = algorithm(n_components=components,\n",
    "                           kernel=args['kernel'],\n",
    "                           degree=args['degree'],\n",
    "                           gamma=args['gamma'])\n",
    "    else:\n",
    "        result = algorithm(n_components=components) #, kernel='rbf', degree=4, gamma=0.4)\n",
    "\n",
    "    ## ICA - Using 3 dimensions for visual analysis\n",
    "    X3dim = result.fit_transform(X)\n",
    "    print(f'New dimensions = {X3dim.shape}')\n",
    "\n",
    "    QUALITY_1 = 5\n",
    "    QUALITY_2 = 6\n",
    "    QUALITY_3 = 7\n",
    "    qualities = [QUALITY_1,\n",
    "                 QUALITY_2,\n",
    "                 QUALITY_3]\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for col, j in zip (['r', 'c', 'y'], qualities):\n",
    "        ax.scatter(X3dim[y == j, 0],\n",
    "                   X3dim[y == j, 1],\n",
    "                   X3dim[y == j, 2],\n",
    "                   alpha=0.4,\n",
    "                   marker='.',\n",
    "                   label=j,\n",
    "                   color=col,\n",
    "                   lw=2,\n",
    "                   s=60)\n",
    "    plt.legend(title='Quality', loc='best')\n",
    "    # plt.xlabel('X1', fontsize=12)\n",
    "    # plt.ylabel('X2', fontsize=12)\n",
    "    # plt.ylabel('other', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    ## Return the results to be used in other algorithms\n",
    "    return X3dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run KMeans with X\n",
    "kmeans(X, 3, 3, 'Original Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build PCA Charts\n",
    "Xpca = CA_Algorithm_2D(PCA,\n",
    "                       False,\n",
    "                       True,\n",
    "                       'Scatterplot for PCA reduction to 2D',\n",
    "                       components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(PCA,\n",
    "                False,\n",
    "                'Scatterplot for PCA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xpca, 2, 2, 'PCA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build Kernel PCA Charts\n",
    "Xkpca = CA_Algorithm_2D(KernelPCA,\n",
    "                        True,\n",
    "                        True,\n",
    "                        'Scatterplot for Kernel PCA reduction to 2D',\n",
    "                        components=N_COMPONENTS,\n",
    "                        kernel='rbf',\n",
    "                        degree=4,\n",
    "                        gamma=0.4)\n",
    "CA_Algorithm_3D(KernelPCA,\n",
    "                True,\n",
    "                'Scatterplot for Kernel PCA reduction to 3D',\n",
    "                components=N_COMPONENTS,\n",
    "                kernel='rbf',\n",
    "                degree=4,\n",
    "                gamma=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xkpca, 0.5, 0.5, 'Kernel PCA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build ICA Charts\n",
    "Xica = CA_Algorithm_2D(FastICA,\n",
    "                       False,\n",
    "                       False,\n",
    "                       'Scatterplot for ICA reduction to 2D',\n",
    "                       components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(FastICA,\n",
    "                False,\n",
    "                ' Scatterplot for ICA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xica, 3, 0.1, 'ICA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build SparsePCA Charts\n",
    "Xspca = CA_Algorithm_2D(SparsePCA,\n",
    "                        False,\n",
    "                        False,\n",
    "                        'Scatterplot for SparsePCA reduction to 2D',\n",
    "                        components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(SparsePCA,\n",
    "                False,\n",
    "                'Scatterplot for SparsePCA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xspca, 2.5, 4, 'Sparse ICA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import multivariate_normal\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Included following due to internet certificate problems\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Move to correct folder for server.  Can remove before sending\n",
    "# os.chdir('/home/poblivsig/Dropbox/horses2')\n",
    "os.chdir('/Users/paullivesey/Dropbox/2. Personal/3. Projects/Python/unsupervised')\n",
    "\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Open the pre-processed csv\n",
    "df = pd.read_csv('data/winequality-red.csv')\n",
    "# df = pd.read_csv('data/phishing.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Get info about wine\n",
    "print(f'Shape\\n\\n{df.shape}')\n",
    "print(f'Columns\\n\\n{df.columns}')\n",
    "print(f'dtypes\\n\\n{df.dtypes}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(f'Description\\n\\n{df.describe()}')\n",
    "print(f'Info:\\n{df.info}')\n",
    "print(f'Check out the sample: {df.sample(n=1)}')\n",
    "pd.set_option('display.max_columns', 5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y = df['quality']\n",
    "X = df.drop('quality', axis=1)\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10\n",
    "GM_N_CLUSTERS = 10\n",
    "INIT = 'k-means++'\n",
    "N_INIT = 10\n",
    "KM_MAX_ITERS = 300\n",
    "TOLERANCE = 1e-4\n",
    "PC_DISTANCES = True\n",
    "KM_VERBOSE = 0\n",
    "KM_RANDOM_STATE = 42\n",
    "ALGORITHM = 'full'\n",
    "FEATURE_1_TO_PLOT = 8\n",
    "FEATURE_2_TO_PLOT = 9\n",
    "N_COMPONENTS = 11"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Constants for different algorithms\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale the features (attributes)\n",
    "scaler = RobustScaler()\n",
    "X = scaler.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualization of the raw data\n",
    "sns.set_context('notebook')\n",
    "plt.style.use('bmh')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_CLUSTERS = 10\n",
    "silhouettes = []\n",
    "\n",
    "## Loop through the cluster numbers and output silhouette\n",
    "## and elbow charts\n",
    "\n",
    "for n_cluster in range(2, N_CLUSTERS+1):\n",
    "    km = KMeans(n_clusters=n_cluster,\n",
    "                init=INIT,\n",
    "                n_init=N_INIT,\n",
    "                max_iter=KM_MAX_ITERS,\n",
    "                tol=TOLERANCE,\n",
    "                precompute_distances=PC_DISTANCES,\n",
    "                verbose=KM_VERBOSE,\n",
    "                random_state=KM_RANDOM_STATE,\n",
    "                algorithm=ALGORITHM)\n",
    "\n",
    "    y_pred = km.fit_predict(X)\n",
    "\n",
    "    ### Print some stats\n",
    "    print(f'inertia = {km.inertia_}')\n",
    "    silhouettes.append(silhouette_score(X, km.labels_, metric='euclidean'))\n",
    "    # print(f'silhouette score = {s_score:.3f}')\n",
    "\n",
    "print(f'silhouettes = {silhouettes}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.lineplot(x=np.arange(2 ,N_CLUSTERS+1), y=silhouettes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def kmeans(Xk, xlim, ylim, data_title):\n",
    "    print(len(np.arange(2, N_CLUSTERS+1)))\n",
    "    print(len(silhouettes))\n",
    "\n",
    "    #%\n",
    "    #****** Run the KMeans and create Silhouette and scatter ******\n",
    "    clusters = np.arange(2, N_CLUSTERS+1)\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    ## Borrowed from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "    for cluster in clusters:\n",
    "        ## Build the plots\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        ## The plot on x for the silhouette coeffients ranges from -1 to +1\n",
    "        ax1.set_xlim([-0.25, 1])\n",
    "        ## The plot on y has to include all of the shapes with their values sorted\n",
    "        ax1.set_ylim([0, len(X) + (cluster + 1) * 10])\n",
    "        fig.set_size_inches(16, 6)\n",
    "\n",
    "        ## Now run the clustering algorithm itself\n",
    "        km = KMeans(n_clusters=cluster,\n",
    "                    init=INIT,\n",
    "                    n_init=N_INIT,\n",
    "                    max_iter=KM_MAX_ITERS,\n",
    "                    tol=TOLERANCE,\n",
    "                    precompute_distances=PC_DISTANCES,\n",
    "                    verbose=KM_VERBOSE,\n",
    "                    random_state=KM_RANDOM_STATE,\n",
    "                    algorithm=ALGORITHM)\n",
    "\n",
    "        print(f'inertia for {cluster} clusters = {km.inertia_}')\n",
    "        y_pred = km.fit_predict(Xk)\n",
    "        cluster_lbls = km.labels_\n",
    "\n",
    "        ## Get the silhoueete score which gives a basic silhouette_score\n",
    "        ## for the run.  Store away from plotting later\n",
    "        silhouette_average = silhouette_score(Xk, y_pred)\n",
    "        silhouette_scores[cluster] = silhouette_average\n",
    "        # What is the silhouette score for each instance?\n",
    "        sample_silhouette_scores = silhouette_samples(Xk, y_pred)\n",
    "\n",
    "        lower_y = 10\n",
    "        for j in range(cluster):\n",
    "            # Group together the silhouette coefficients for cluster i\n",
    "            # and the sort them from largest to smallest\n",
    "            j_cluster_coeffs = sample_silhouette_scores[y_pred == j]\n",
    "            j_cluster_coeffs.sort()\n",
    "\n",
    "            ## Get bottom of cluster shape for chart\n",
    "            upper_y = lower_y + j_cluster_coeffs.shape[0]\n",
    "            colour = cm.rainbow(float(j) / cluster)\n",
    "\n",
    "            ## Draw the cluster shape\n",
    "            ax1.fill_betweenx(np.arange(lower_y, upper_y),\n",
    "                             0, j_cluster_coeffs,\n",
    "                             facecolor=colour, edgecolor=colour, alpha=0.7)\n",
    "            ax1.text(-0.05, lower_y + 0.5 *j_cluster_coeffs.shape[0], str(j))\n",
    "\n",
    "            # Get the next clusters position\n",
    "            lower_y = upper_y + 10\n",
    "\n",
    "        ## Draw the average silhouette score line.\n",
    "        ax1.axvline(x=silhouette_average, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "        ## Set the title and labels\n",
    "        ax1.set_xlabel('Silhouette Coefficient', fontsize=11)\n",
    "        ax1.set_ylabel('Cluster', fontsize=11)\n",
    "\n",
    "        ax1.set_title(f'Silhouette Diagram for {cluster} Clusters', fontsize=14)\n",
    "\n",
    "        ## Create 2D scatterplot for the clusters created above\n",
    "        ax2.scatter( Xk[:, FEATURE_1_TO_PLOT],\n",
    "                     Xk[:, FEATURE_2_TO_PLOT],\n",
    "                     marker='.',\n",
    "                     s=30,\n",
    "                     lw=0,\n",
    "                     alpha=0.5,\n",
    "                     c=cm.rainbow(km.labels_.astype(float) / cluster),\n",
    "                     edgecolor='k')\n",
    "        ax2.scatter(km.cluster_centers_[:, 0],\n",
    "                    km.cluster_centers_[:, 1],\n",
    "                    marker='o',\n",
    "                    c='white',\n",
    "                    alpha=1,\n",
    "                    s=180,\n",
    "                    edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(km.cluster_centers_):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "        ax2.set_xlim([0, xlim])\n",
    "        ax2.set_xlim([0, ylim])\n",
    "        ax2.set_xlabel('1st Feature')\n",
    "        ax2.set_ylabel('2nd Feature')\n",
    "        ax2.set_title(f'{cluster} Cluster data scatterplot for 2 Features.', fontsize=14)\n",
    "\n",
    "        plt.suptitle((f'{data_title} K-Means Clustering on Sample Data with {cluster} Clusters'),\n",
    "                     fontsize=15)\n",
    "\n",
    "    # print(f'silhouette scores = {silhouette_scores}')\n",
    "    plt.show()\n",
    "    print(f'silhouettes = {silhouettes}')\n",
    "\n",
    "    plt.title(f'Silhouette Line-plot for {data_title}', fontsize=14)\n",
    "    plt.xlabel('Clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.plot(np.arange(1, 10),silhouettes)\n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#g****** Run the GAUSSIAN MIXTURE and create Silhouette and scatter ******\n",
    "clusters = np.arange(2, N_CLUSTERS+1)\n",
    "silhouette_scores = {}\n",
    "bics = []\n",
    "aics = []\n",
    "\n",
    "## Borrowed from https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "for cluster in clusters:\n",
    "    ## Build the plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    ## The plot on x for the silhouette coeffients ranges from -1 to +1\n",
    "    ax1.set_xlim([-0.25, 1])\n",
    "    ## The plot on y has to include all of the shapes with their values sorted\n",
    "    ax1.set_ylim([0, len(X) + (cluster + 1) * 10])\n",
    "    fig.set_size_inches(16, 6)\n",
    "\n",
    "    ## Now run the clustering algorithm itself\n",
    "    gm = GaussianMixture(n_components=cluster )\n",
    "    y_pred_gm = gm.fit_predict(X)\n",
    "    bics.append(gm.bic(X))\n",
    "    aics.append(gm.aic(X))\n",
    "    # km = KMeans(n_clusters=cluster,\n",
    "    #             init=INIT,\n",
    "    #             n_init=N_INIT,\n",
    "    #             max_iter=KM_MAX_ITERS,\n",
    "    #             tol=TOLERANCE,\n",
    "    #             precompute_distances=PC_DISTANCES,\n",
    "    #             verbose=KM_VERBOSE,\n",
    "    #             random_state=KM_RANDOM_STATE,\n",
    "    #             algorithm=ALGORITHM)\n",
    "\n",
    "    # y_pred = km.fit_predict(X)\n",
    "    cluster_lbls = np.unique(y_pred_gm[:cluster]) #gm.labels_\n",
    "\n",
    "    ## Get the silhouette score which gives a basic silhouette_score\n",
    "    ## for the run.  Store away from plotting later\n",
    "    silhouette_average = silhouette_score(X, y_pred_gm)\n",
    "    silhouette_scores[cluster] = silhouette_average\n",
    "    # What is the silhouette score for each instance?\n",
    "    sample_silhouette_scores = silhouette_samples(X, y_pred_gm)\n",
    "\n",
    "    lower_y = 10\n",
    "    for j in range(cluster):\n",
    "        # Group together the silhouette coefficients for cluster i\n",
    "        # and the sort them from largest to smallest\n",
    "        j_cluster_coeffs = sample_silhouette_scores[y_pred_gm == j]\n",
    "        j_cluster_coeffs.sort()\n",
    "\n",
    "        ## Get bottom of cluster shape for chart\n",
    "        upper_y = lower_y + j_cluster_coeffs.shape[0]\n",
    "        colour = cm.rainbow(float(j) / cluster)\n",
    "\n",
    "        ## Draw the cluster shape\n",
    "        ax1.fill_betweenx(np.arange(lower_y, upper_y),\n",
    "                         0, j_cluster_coeffs,\n",
    "                         facecolor=colour, edgecolor=colour, alpha=0.7)\n",
    "        ax1.text(-0.05, lower_y + 0.5 *j_cluster_coeffs.shape[0], str(j))\n",
    "\n",
    "        # Get the next clusters position\n",
    "        lower_y = upper_y + 10\n",
    "\n",
    "    ## Draw the average silhouette score line.\n",
    "    ax1.axvline(x=silhouette_average, color=\"green\", linestyle=\"--\")\n",
    "\n",
    "    ## Set the title and labels\n",
    "    ax1.set_xlabel('Silhouette Coefficient', fontsize=11)\n",
    "    ax1.set_ylabel('Cluster', fontsize=11)\n",
    "\n",
    "    ax1.set_title(f'Silhouette Diagram for {cluster} Clusters', fontsize=14)\n",
    "\n",
    "    ## Create 2D scatterplot for the clusters created above\n",
    "    ax2.scatter( X[:, FEATURE_1_TO_PLOT],\n",
    "                 X[:, FEATURE_2_TO_PLOT],\n",
    "                 marker='.',\n",
    "                 s=30,\n",
    "                 lw=0,\n",
    "                 alpha=0.5,\n",
    "                 c=cm.rainbow(km.labels_.astype(float) / cluster),\n",
    "                 edgecolor='k')\n",
    "    ## Find centers for Gaussian clusters (choosing the points with\n",
    "    ## the maximal density to represent its cluster.\n",
    "    centers = np.empty(shape=(gm.n_components, X.shape[1]))\n",
    "    for i in range(gm.n_components):\n",
    "        density = multivariate_normal(cov=gm.covariances_[i],\n",
    "                                      mean=gm.means_[i]).logpdf(X)\n",
    "        centers[i, :] = X[np.argmax(density)]\n",
    "\n",
    "    ax2.scatter(centers[:, 0],\n",
    "                centers[:, 1],\n",
    "                marker='o',\n",
    "                c=\"white\",\n",
    "                alpha=1,\n",
    "                s=180,\n",
    "                edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0],\n",
    "                    c[1],\n",
    "                    marker='$%d$' % i,\n",
    "                    alpha=1,\n",
    "                    s=50,\n",
    "                    edgecolor='k')\n",
    "    ax2.set_xlim([0,5])\n",
    "    ax2.set_xlabel(\"1st Feature\")\n",
    "    ax2.set_ylabel(\"2nd Feature\")\n",
    "    ax2.set_title(\"Clustered data scatterplot for 2 Features.\", fontsize=14)\n",
    "\n",
    "    plt.suptitle((f'K-Means Clustering on Sample Data with {cluster} Clusters'),\n",
    "                 fontsize=14)\n",
    "\n",
    "# print(f'silhouette scores = {silhouette_scores}')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Create the Silhouette Score Chart\n",
    "sns.lineplot(x=clusters, y=list(silhouette_scores.values()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bics = []\n",
    "aics = []\n",
    "clusters = np.arange(2, N_CLUSTERS+1)\n",
    "\n",
    "for n_cluster in clusters:\n",
    "    gm = GaussianMixture(n_components=n_cluster )\n",
    "    y_clust_gm = gm.fit_predict(X)\n",
    "    bics.append(gm.bic(X))\n",
    "    aics.append(gm.aic(X))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Create AIC and BIC chart\n",
    "plt.plot(clusters, bics, label='bic')\n",
    "plt.plot(clusters, aics, label='aic')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Find the Bayesian Gaussian Mixture\n",
    "bgm = BayesianGaussianMixture ( n_components = 10 , n_init = 10 )\n",
    "bgm.fit ( X )\n",
    "print(np.round ( bgm.weights_ , 2 ))\n",
    "\n",
    "##"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Calculate the best PCS dimensions\n",
    "pca_res = KernelPCA(n_components=6, kernel='rbf', degree=4, gamma=0.1)\n",
    "pca_res.fit(X)\n",
    "d = np.argmax(np.cumsum(pca_res.explained_variance_ratio_) >=  0.95) + 1\n",
    "print(f'optimal PCA dimensions = {d}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Use the optimal dimension to calculate the principal components...\n",
    "\n",
    "pca = KernelPCA(n_components=5)\n",
    "X2dim = pca.fit_transform(X)\n",
    "print(f'New dimensions = {X2dim.shape}')\n",
    "print(f'principal components = {pca.explained_variance_ratio_}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Plot different dimensions against the explained variance\n",
    "NO_DIMS_TO_CHECK = 11\n",
    "dimensions = np.arange(1, NO_DIMS_TO_CHECK)\n",
    "expl_variances = []\n",
    "\n",
    "for dimension in dimensions:\n",
    "    pca = PCA(n_components=dimension)\n",
    "    pca.fit(X)\n",
    "    expl_variances.append(np.sum(pca.explained_variance_ratio_))\n",
    "\n",
    "print(f'expl_variances = \\n{expl_variances}')\n",
    "print(f'dimensions = \\n{dimensions}')\n",
    "plt.title('Explained Variance vs. Dimensions', fontsize=14)\n",
    "plt.xlabel('No. of Dimensions', fontsize=12)\n",
    "plt.ylabel('Explained Variance', fontsize=12)\n",
    "plt.axvline(6, color='r', linestyle='dotted')\n",
    "plt.axvline(6, color='r', linestyle='dotted')\n",
    "plt.axhline(0.938, color='r', linestyle='dotted')\n",
    "sns.lineplot(dimensions, expl_variances)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CA_Algorithm_2D(algorithm, KPCA, PCA_TYPE, title, components, **args):\n",
    "    if KPCA:\n",
    "        result = algorithm(n_components=components,\n",
    "                           kernel=args['kernel'],\n",
    "                           degree=args['degree'],\n",
    "                           gamma=args['gamma'])\n",
    "    else:\n",
    "        result = algorithm(n_components=components) #, kernel='rbf', degree=4, gamma=0.4)\n",
    "\n",
    "    X2dim = result.fit_transform(X)\n",
    "    print(f'New dimensions = {X2dim.shape}')\n",
    "\n",
    "    QUALITY_1 = 5\n",
    "    QUALITY_2 = 6\n",
    "    QUALITY_3 = 7\n",
    "\n",
    "    qualities = [QUALITY_1,\n",
    "                 QUALITY_2,\n",
    "                 QUALITY_3]\n",
    "    plt.figure()\n",
    "    for col, j in zip (['r', 'c', 'y'], qualities):\n",
    "        plt.scatter(X2dim[y == j, 0],\n",
    "                    X2dim[y == j, 1],\n",
    "                    alpha=0.4,\n",
    "                    marker='.',\n",
    "                    label=j,\n",
    "                    color=col, #['r', 'c'],\n",
    "                    lw=2,\n",
    "                    s=40)\n",
    "    plt.legend(title='Quality', loc='best')\n",
    "    plt.xlabel('X1', fontsize=12)\n",
    "    plt.ylabel('X2', fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "    if PCA_TYPE:\n",
    "        d = np.argmax(np.cumsum(result.explained_variance_ratio_) >=  0.95) + 1\n",
    "        print(f'optimal PCA dimensions = {d}')\n",
    "\n",
    "    ## Return the results to be used in other algorithms\n",
    "    return X2dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def CA_Algorithm_3D(algorithm, KPCA, title, components, **args):\n",
    "    if KPCA:\n",
    "        result = algorithm(n_components=components,\n",
    "                           kernel=args['kernel'],\n",
    "                           degree=args['degree'],\n",
    "                           gamma=args['gamma'])\n",
    "    else:\n",
    "        result = algorithm(n_components=components) #, kernel='rbf', degree=4, gamma=0.4)\n",
    "\n",
    "    ## ICA - Using 3 dimensions for visual analysis\n",
    "    X3dim = result.fit_transform(X)\n",
    "    print(f'New dimensions = {X3dim.shape}')\n",
    "\n",
    "    QUALITY_1 = 5\n",
    "    QUALITY_2 = 6\n",
    "    QUALITY_3 = 7\n",
    "    qualities = [QUALITY_1,\n",
    "                 QUALITY_2,\n",
    "                 QUALITY_3]\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    for col, j in zip (['r', 'c', 'y'], qualities):\n",
    "        ax.scatter(X3dim[y == j, 0],\n",
    "                   X3dim[y == j, 1],\n",
    "                   X3dim[y == j, 2],\n",
    "                   alpha=0.4,\n",
    "                   marker='.',\n",
    "                   label=j,\n",
    "                   color=col,\n",
    "                   lw=2,\n",
    "                   s=60)\n",
    "    plt.legend(title='Quality', loc='best')\n",
    "    # plt.xlabel('X1', fontsize=12)\n",
    "    # plt.ylabel('X2', fontsize=12)\n",
    "    # plt.ylabel('other', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.show()\n",
    "\n",
    "    ## Return the results to be used in other algorithms\n",
    "    return X3dim"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Run KMeans with X\n",
    "kmeans(X, 3, 3, 'Original Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build PCA Charts\n",
    "Xpca = CA_Algorithm_2D(PCA,\n",
    "                       False,\n",
    "                       True,\n",
    "                       'Scatterplot for PCA reduction to 2D',\n",
    "                       components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(PCA,\n",
    "                False,\n",
    "                'Scatterplot for PCA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xpca, 2, 2, 'PCA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build Kernel PCA Charts\n",
    "Xkpca = CA_Algorithm_2D(KernelPCA,\n",
    "                        True,\n",
    "                        True,\n",
    "                        'Scatterplot for Kernel PCA reduction to 2D',\n",
    "                        components=N_COMPONENTS,\n",
    "                        kernel='rbf',\n",
    "                        degree=4,\n",
    "                        gamma=0.4)\n",
    "CA_Algorithm_3D(KernelPCA,\n",
    "                True,\n",
    "                'Scatterplot for Kernel PCA reduction to 3D',\n",
    "                components=N_COMPONENTS,\n",
    "                kernel='rbf',\n",
    "                degree=4,\n",
    "                gamma=0.4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xkpca, 0.5, 0.5, 'Kernel PCA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build ICA Charts\n",
    "Xica = CA_Algorithm_2D(FastICA,\n",
    "                       False,\n",
    "                       False,\n",
    "                       'Scatterplot for ICA reduction to 2D',\n",
    "                       components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(FastICA,\n",
    "                False,\n",
    "                ' Scatterplot for ICA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xica, 3, 0.1, 'ICA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build SparsePCA Charts\n",
    "Xspca = CA_Algorithm_2D(SparsePCA,\n",
    "                        False,\n",
    "                        False,\n",
    "                        'Scatterplot for SparsePCA reduction to 2D',\n",
    "                        components=N_COMPONENTS)\n",
    "\n",
    "CA_Algorithm_3D(SparsePCA,\n",
    "                False,\n",
    "                'Scatterplot for SparsePCA reduction to 3D',\n",
    "                components=N_COMPONENTS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Run KMeans again with reduced dimension data\n",
    "kmeans(Xspca, 2.5, 4, 'Sparse ICA Data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(cross_val_score(decisiontreeclassifier(criterion = 'gini', random_state = 0), X, y, cv=5))\n",
    "def gridsearch(estimator, param_grid, cv, scoring_metric):\n",
    "    scorer = sklearn.metrics.make_scorer(sklearn.metrics.f1_score, average = 'weighted')\n",
    "    clf = gridsearchcv(estimator=estimator,\n",
    "                       param_grid=param_grid,\n",
    "                       n_jobs=-1,\n",
    "                       cv=cv,\n",
    "                       return_train_score=True,\n",
    "                       scoring=scorer,\n",
    "                       verbose=3)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    ### Output the results\n",
    "    print(f'Best parameters: {clf.best_params_}')\n",
    "    print(f'Best score: {clf.best_score_}')\n",
    "    best_estimate = clf.best_estimator_\n",
    "    print(best_estimate)\n",
    "\n",
    "    ## Now we have found the best parameters, use them...\n",
    "    best_estimate.fit(X,y)\n",
    "\n",
    "    predictor = best_estimate.predict(X)\n",
    "    mse = mean_squared_error(predictor, y)\n",
    "    r2 = r2_score(predictor, y)\n",
    "    print(f'Training Mean Square Error: {mse:.2f}')\n",
    "    print(f'Training R2: {r2:.2f}')\n",
    "\n",
    "    y_predictor = best_estimate.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predictor)\n",
    "    r2 = r2_score(y_test, y_predictor)\n",
    "    print(f'Testing Mean Square Error: {mse:.2f}')\n",
    "    print(f'Testing R2: {r2:.2f}')\n",
    "    print('blah')\n",
    "\n",
    "    return best_estimate, y_predictor\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cm_and_class_rep(X_test, y_test, y_predictor, best_estimate):\n",
    "    confusion_matrix(y_test, y_predictor)\n",
    "    cm = plot_confusion_matrix(best_estimate,\n",
    "                               X_test,\n",
    "                               y_test,\n",
    "                               cmap=plt.cm.Blues,\n",
    "                               normalize='true' )\n",
    "    plt.show(cm)\n",
    "    plt.show()\n",
    "    print(classification_report(y_test, y_predictor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_learning_curve(estimator_1,\n",
    "                        estimator_1_name,\n",
    "                        estimator_2,\n",
    "                        estimator_2_name,\n",
    "                        estimator_3,\n",
    "                        estimator_3_name,\n",
    "                        X,\n",
    "                        y,\n",
    "                        cv,\n",
    "                        train_max,\n",
    "                        title):\n",
    "\n",
    "    # Set plot size\n",
    "    plt.figure(figsize=(7,5))\n",
    "\n",
    "    ###################################\n",
    "    # Do the 1st curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_1,\n",
    "                                 X,\n",
    "                                 y,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_1_name,\n",
    "             color='blue')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_1_name,\n",
    "             color='cornflowerblue')\n",
    "\n",
    "\n",
    "    ###################################\n",
    "    # Do the 2nd curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_2,\n",
    "                                 X,\n",
    "                                 y,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_2_name,\n",
    "             color='green')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_2_name,\n",
    "             color='springgreen')\n",
    "\n",
    "    ###################################\n",
    "    # Do the 3rd curve\n",
    "    sizes, \\\n",
    "    training_scores, \\\n",
    "    testing_scores, \\\n",
    "    fit_times, \\\n",
    "    score_times = learning_curve(estimator_3,\n",
    "                                 X,\n",
    "                                 y,\n",
    "                                 cv=cv,\n",
    "                                 scoring='accuracy',\n",
    "                                 return_times=True,\n",
    "                                 train_sizes=np.arange(1, train_max, 10))\n",
    "\n",
    "    # Mean of training scores\n",
    "    mean_training = np.mean(training_scores, axis=1)\n",
    "\n",
    "    # Mean of testing scores\n",
    "    mean_testing = np.mean(testing_scores, axis=1)\n",
    "\n",
    "    # Do the best lines\n",
    "    plt.plot(sizes,\n",
    "             mean_training,\n",
    "             '--',\n",
    "             label='Training Score - ' + estimator_3_name,\n",
    "             color='red')\n",
    "    plt.plot(sizes,\n",
    "             mean_testing,\n",
    "             label='Cross Validation Score - ' + estimator_3_name,\n",
    "             color='lightcoral')\n",
    "\n",
    "    # Do the final plots\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Training Set Size'), plt.ylabel('Accuracy'), plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig('data/charts/wine_learning_curve.png')\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw Validation Curve\n",
    "def draw_validation_curve(estimator, X, y, cv, param_name, param_range, title, xlabel):\n",
    "\n",
    "    train_scores, test_scores = validation_curve(estimator,\n",
    "                                                 X,\n",
    "                                                 y,\n",
    "                                                 param_name=param_name,\n",
    "                                                 param_range=param_range,\n",
    "                                                 cv=cv,\n",
    "                                                 scoring='accuracy',\n",
    "                                                 n_jobs=-1)\n",
    "\n",
    "    # Mean from the scores for the training set\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "\n",
    "    # Mean from the scores for the test set\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    # Create plot for training\n",
    "    plt.plot(param_range,\n",
    "             train_mean,\n",
    "             label=\"Training score\",\n",
    "             color=\"blue\")\n",
    "\n",
    "    # Create plot for testing\n",
    "    plt.plot(param_range,\n",
    "             test_mean,\n",
    "             label=\"Cross-validation score\",\n",
    "             color=\"red\")\n",
    "\n",
    "    # Build plot\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(\"Accuracy Score\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/charts/wine_validation_curve.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# MLP learning curve\n",
    "tuned_hidden_layer_sizes = 7\n",
    "tuned_activation = 'relu'\n",
    "tuned_max_iter = 1500\n",
    "tuned_alpha = 0.001\n",
    "tuned_learning_rate = 'adaptive'\n",
    "\n",
    "draw_learning_curve(mlp(hidden_layer_sizes=2,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 2',\n",
    "                    mlp(hidden_layer_sizes=7,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 7',\n",
    "                    mlp(hidden_layer_sizes=15,\n",
    "                        activation=tuned_activation,\n",
    "                        max_iter=tuned_max_iter,\n",
    "                        alpha=tuned_alpha,\n",
    "                        learning_rate=tuned_learning_rate),\n",
    "                    'hidden_layer_size = 15',\n",
    "                    X,\n",
    "                    y,\n",
    "                    cv=8,\n",
    "                    train_max=500,\n",
    "                    title = 'MLP Red Wine Learning Curve Training Set Size vs. hidden layer size')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Draw MLP Validation Curve\n",
    "draw_validation_curve(mlp(hidden_layer_sizes=tuned_hidden_layer_sizes,\n",
    "                          activation=tuned_activation,\n",
    "                          max_iter=tuned_max_iter,\n",
    "                          alpha=tuned_alpha,\n",
    "                          learning_rate=tuned_learning_rate),\n",
    "                      X,\n",
    "                      y,\n",
    "                      cv=8,\n",
    "                      param_name='hidden_layer_sizes',\n",
    "                      param_range=np.arange(0, MAX_LAYER_SIZE),\n",
    "                      title='MLP Red Wine Validation Curve for Hidden Layer Sizes',\n",
    "                      xlabel='Hidden Layer Sizes')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4b96a2e0",
   "language": "python",
   "display_name": "PyCharm (unsupervised)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}